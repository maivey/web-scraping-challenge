{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Function (scrape_mars.py in full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert  Jupyter notebook into a Python script called scrape_mars.py with a function called scrape that will execute all of your scraping code from above and return one Python dictionary containing all of the scraped data.\n",
    "def scrape():\n",
    "    # Import dependencies\n",
    "    from splinter import Browser\n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium import webdriver\n",
    "    import pandas as pd\n",
    "    from IPython.display import display \n",
    "    from IPython.display import display_html\n",
    "\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "    #-----------NASA MARS NEWS---------------\n",
    "    # Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later.\n",
    "    url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find latest News Title\n",
    "    titles = soup.find_all('div',class_='content_title')\n",
    "    news_title = titles[0].text\n",
    "\n",
    "    # Find latest News Paragraph Text\n",
    "    paragraphs = soup.find_all('div',class_='article_teaser_body')\n",
    "    news_p = paragraphs[0].text\n",
    "\n",
    "    #-------------JPL MARS SPACE IMAGES - FEATURED IMAGE-------------\n",
    "    # Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url.\n",
    "    jpl_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    jpl_short_url = jpl_url.split('/spaceimages')[0]\n",
    "    browser.visit(jpl_url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    large_url = soup.find_all('a', class_=\"button fancybox\")[0]['data-link']\n",
    "    large_entire_url = jpl_short_url + large_url\n",
    "    browser.visit(large_entire_url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    large_size_partial = soup.find_all('img', class_='main_image')[0]['src']\n",
    "    featured_image_url = jpl_short_url + large_size_partial\n",
    "    # featured_image_url = jpl_url+soup.find_all('a',id=\"full_image\")[0]['data-fancybox-href']\n",
    "    # # featured_image_url\n",
    "\n",
    "    #----------- MARS WEATHER---------------\n",
    "    twitter_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(twitter_url)\n",
    "    html = driver.page_source\n",
    "    mars_weather = driver.find_element_by_class_name('TweetTextSize').text\n",
    "\n",
    "    #-------------- MARS FACTS---------------\n",
    "    table_url = 'https://space-facts.com/mars/'\n",
    "    browser.visit(table_url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    df_table = pd.read_html(html)[0]\n",
    "    df_table = df_table.rename(columns={0:'description',1:'value'})\n",
    "    df_table = df_table.set_index('description')\n",
    "    html_table_string = df_table.to_html()\n",
    "\n",
    "    #----------- MARS HEMISPHERES---------------\n",
    "    hemi_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "    hemi_short_url = hemi_url.split('/search')[0]\n",
    "    browser.visit(hemi_url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    link_test = browser.find_by_css('h3')\n",
    "    hemi_names = [link.value for link in link_test]\n",
    "    hrefs = [img.a['href'] for img in soup.find_all('div',class_='description')]\n",
    "    clicked_urls = [(hemi_short_url + href) for href in hrefs]\n",
    "    img_urls = []\n",
    "    for url in clicked_urls:\n",
    "        browser.visit(url)\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        img_urls.append(hemi_short_url + soup.find_all('img',class_='wide-image')[0]['src'])\n",
    "\n",
    "    hemi_1 = {'title':hemi_names[0],'img_url':img_urls[0]}\n",
    "    hemi_2= {'title':hemi_names[1],'img_url':img_urls[1]}\n",
    "    hemi_3 = {'title':hemi_names[2],'img_url':img_urls[2]}\n",
    "    hemi_4 = {'title':hemi_names[3],'img_url':img_urls[3]}\n",
    "    hemisphere_image_urls = [hemi_1, hemi_2, hemi_3, hemi_4]\n",
    "\n",
    "    # Return one python dictionary containing all of the scraped data\n",
    "    main_dict = {'news_title':news_title}\n",
    "    main_dict['news_p']=news_p\n",
    "    main_dict['featured_image_url']=featured_image_url\n",
    "    main_dict['mars_weather']=mars_weather\n",
    "    main_dict['html_table_string']=html_table_string\n",
    "    main_dict['hemisphere_image_urls']=hemisphere_image_urls\n",
    "\n",
    "    # close the browser\n",
    "    browser.quit()\n",
    "\n",
    "    return main_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script for app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from flask import Flask, render_template, redirect\n",
    "import scrape_mars\n",
    "# Module used to connect Python with MongoDb\n",
    "# import pymongo\n",
    "from flask_pymongo import PyMongo\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "mongo = PyMongo(app, uri=\"mongodb://localhost:27017/marsDB\")\n",
    "\n",
    "\n",
    "#Routes\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    mars_data = mongo.db.collection.find_one()\n",
    "\n",
    "    return render_template(\"index.html\", mars_data = mars_data)\n",
    "\n",
    "# #/scrape\n",
    "@app.route(\"/scrape\")\n",
    "def scrape_data():\n",
    "    # return scraped_data\n",
    "    scraped_data = scrape_mars.scrape()\n",
    "    mongo.db.collection.update({}, scraped_data, upsert=True)\n",
    "    return redirect(\"/\", code=302)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
